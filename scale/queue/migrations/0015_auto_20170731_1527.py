# -*- coding: utf-8 -*-
# Generated by Django 1.11 on 2017-07-31 19:27
from __future__ import unicode_literals

import math

from django.db import migrations
from django.utils.timezone import now

from job.configuration.configurators import QueuedExecutionConfigurator
from job.configuration.data.job_data import JobData
from job.configuration.interface.job_interface import JobInterface
from node.resources.node_resources import NodeResources
from node.resources.resource import Disk, Mem

MIN_MEM = 128.0
MIN_DISK = 0.0

def get_resources(job):
    """Returns the resources required for this job

    :returns: The required resources
    :rtype: :class:`node.resources.node_resources.NodeResources`
    """

    resources = job.job_type.get_resources()

    # Calculate memory required in MiB rounded up to the nearest whole MiB
    multiplier = job.job_type.mem_mult_required
    const = job.job_type.mem_const_required
    disk_in_required = job.disk_in_required
    if not disk_in_required:
        disk_in_required = 0.0
    memory_mb = long(math.ceil(multiplier * disk_in_required + const))
    memory_required = max(memory_mb, MIN_MEM)

    # Calculate output space required in MiB rounded up to the nearest whole MiB
    multiplier = job.job_type.disk_out_mult_required
    const = job.job_type.disk_out_const_required
    output_size_mb = long(math.ceil(multiplier * disk_in_required + const))
    disk_out_required = max(output_size_mb, MIN_DISK)

    resources.add(NodeResources([Mem(memory_required), Disk(disk_out_required + disk_in_required)]))
    return resources


class Migration(migrations.Migration):

    dependencies = [
        ('job', '0029_auto_20170707_1034'),
        ('product', '0010_auto_20170727_1349'),
        ('storage', '0008_auto_20170609_1443'),
        ('queue', '0014_queue'),
    ]

    def populate_queue(apps, schema_editor):
        from job.configuration.json.execution.exe_config import ExecutionConfiguration

        # Go through all of the queued job models and re-populate the queue table
        when_queued = now()
        Job = apps.get_model('job', 'Job')
        JobExecution = apps.get_model('job', 'JobExecution')
        FileAncestryLink = apps.get_model('product', 'FileAncestryLink')
        Queue = apps.get_model('queue', 'Queue')
        ScaleFile = apps.get_model('storage', 'ScaleFile')
        total_count = Job.objects.filter(status='QUEUED').count()
        print 'Populating new queue table for %s queued jobs' % str(total_count)
        done_count = 0
        batch_size = 1000
        while done_count < total_count:
            percent = (float(done_count) / float(total_count)) * 100.00
            print 'Completed %s of %s jobs (%f%%)' % (done_count, total_count, percent)
            batch_end = done_count + batch_size
            job_qry = Job.objects.filter(status='QUEUED').select_related('job_type', 'job_type_rev')
            job_qry = job_qry.order_by('id')[done_count:batch_end]

            # Query for all input files
            input_files = {}
            input_file_ids = set()
            for job in job_qry:
                input_file_ids.update(JobData(job.data).get_input_file_ids())

            if input_file_ids:
                for input_file in ScaleFile.objects.get_files(input_file_ids):
                    input_files[input_file.id] = input_file

            # Bulk create queue models
            queues = []
            configurator = QueuedExecutionConfigurator(input_files)
            for job in job_qry:
                config = configurator.configure_queued_job(job)

                queue = Queue()
                queue.job_type = job.job_type
                queue.job = job
                queue.exe_num = job.num_exes
                queue.input_file_size = job.disk_in_required if job.disk_in_required else 0.0
                queue.is_canceled = False
                queue.priority = job.priority
                queue.timeout = job.timeout
                queue.interface = JobInterface(job.job_type_rev.interface).get_dict()
                queue.configuration = config.get_dict()
                queue.resources = get_resources(job).get_json().get_dict()
                queue.queued = when_queued
                queues.append(queue)

            if not queues:
                return []

            Queue.objects.bulk_create(queues)
            done_count += batch_size
        print 'All %s jobs completed' % str(total_count)

        total_count = JobExecution.objects.filter(status='QUEUED').count()
        print 'Updating file ancestry links for %s queued job executions' % str(total_count)
        done_count = 0
        batch_size = 1000
        while done_count < total_count:
            percent = (float(done_count) / float(total_count)) * 100.00
            print 'Completed %s of %s queued job executions (%f%%)' % (done_count, total_count, percent)
            batch_end = done_count + batch_size
            job_exe_qry = JobExecution.objects.filter(status='QUEUED').defer('configuration', 'resources')
            job_exe_qry = job_exe_qry.order_by('id')[done_count:batch_end]

            job_exe_ids = []
            for job_exe in job_exe_qry:
                job_exe_ids.append(job_exe.id)

            FileAncestryLink.objects.filter(job_exe_id__in=job_exe_ids).update(job_exe_id=None)

            done_count += batch_size
        print 'All file ancestry links for %s queued job executions completed' % str(total_count)
        print 'Deleting %s queued job executions...' % str(total_count)
        JobExecution.objects.filter(status='QUEUED').delete()

    operations = [
        migrations.RunPython(populate_queue),
    ]
