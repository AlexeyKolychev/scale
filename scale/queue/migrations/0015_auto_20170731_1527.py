# -*- coding: utf-8 -*-
# Generated by Django 1.11 on 2017-07-31 19:27
from __future__ import unicode_literals

from django.db import migrations
from django.utils.timezone import now


class Migration(migrations.Migration):

    dependencies = [
        ('job', '0029_auto_20170707_1034'),
        ('product', '0010_auto_20170727_1349'),
        ('storage', '0008_auto_20170609_1443'),
        ('queue', '0014_queue'),
    ]

    def populate_queue(apps, schema_editor):
        from job.configuration.json.execution.exe_config import ExecutionConfiguration

        # Go through all of the queued job models and re-populate the queue table
        when_queued = now()
        Job = apps.get_model('job', 'Job')
        JobExecution = apps.get_model('job', 'JobExecution')
        FileAncestryLink = apps.get_model('product', 'FileAncestryLink')
        Queue = apps.get_model('queue', 'Queue')
        ScaleFile = apps.get_model('storage', 'ScaleFile')
        total_count = Job.objects.filter(status='QUEUED').count()
        print 'Populating new queue table for %s queued jobs' % str(total_count)
        done_count = 0
        batch_size = 1000
        while done_count < total_count:
            percent = (float(done_count) / float(total_count)) * 100.00
            print 'Completed %s of %s jobs (%f%%)' % (done_count, total_count, percent)
            batch_end = done_count + batch_size
            job_qry = Job.objects.filter(status='QUEUED').select_related('job_type', 'job_type_rev')
            job_qry = job_qry.order_by('id')[done_count:batch_end]

            # Query for all input files
            input_files = {}
            input_file_ids = set()
            for job in job_qry:
                input_file_ids.update(job.get_job_data().get_input_file_ids())

            if input_file_ids:
                for input_file in ScaleFile.objects.get_files(input_file_ids):
                    input_files[input_file.id] = input_file

            # Bulk create queue models
            queues = []
            for job in job_qry:
                config = ExecutionConfiguration()
                config.configure_for_queued_job(job, input_files)

                queue = Queue()
                queue.job_type = job.job_type
                queue.job = job
                queue.exe_num = job.num_exes
                queue.input_file_size = job.disk_in_required if job.disk_in_required else 0.0
                queue.is_canceled = False
                queue.priority = job.priority
                queue.timeout = job.timeout
                queue.interface = job.get_job_interface().get_dict()
                queue.configuration = config.get_dict()
                queue.resources = job.get_resources().get_json().get_dict()
                queue.queued = when_queued
                queues.append(queue)

            if not queues:
                return []

            Queue.objects.bulk_create(queues)
            done_count += batch_size
        print 'All %s jobs completed' % str(total_count)

        total_count = JobExecution.objects.filter(status='QUEUED').count()
        print 'Updating file ancestry links for %s queued job executions' % str(total_count)
        done_count = 0
        batch_size = 1000
        while done_count < total_count:
            percent = (float(done_count) / float(total_count)) * 100.00
            print 'Completed %s of %s queued job executions (%f%%)' % (done_count, total_count, percent)
            batch_end = done_count + batch_size
            job_exe_qry = JobExecution.objects.filter(status='QUEUED').defer('configuration', 'resources')
            job_exe_qry = job_exe_qry.order_by('id')[done_count:batch_end]

            job_exe_ids = []
            for job_exe in job_exe_qry:
                job_exe_ids.append(job_exe.id)

            FileAncestryLink.objects.filter(job_exe_id__in=job_exe_ids).update(job_exe_id=None)

            done_count += batch_size
        print 'All file ancestry links for %s queued job executions completed' % str(total_count)
        print 'Deleting %s queued job executions...' % str(total_count)
        JobExecution.objects.filter(status='QUEUED').delete()

    operations = [
        migrations.RunPython(populate_queue),
    ]
